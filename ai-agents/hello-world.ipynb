{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/pydantic/_internal/_config.py:295: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)\n",
      "/usr/local/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:502: UserWarning: <built-in function callable> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.11/site-packages/crewai_tools/tools/scrapegraph_scrape_tool/scrapegraph_scrape_tool.py:34: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  @validator(\"website_url\")\n",
      "/usr/local/lib/python3.11/site-packages/crewai_tools/tools/selenium_scraping_tool/selenium_scraping_tool.py:26: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  @validator(\"website_url\")\n",
      "/usr/local/lib/python3.11/site-packages/crewai_tools/tools/vision_tool/vision_tool.py:15: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  @validator(\"image_path_url\")\n",
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from crewai_tools import ScrapeWebsiteTool, FileWriterTool, TXTSearchTool\n",
    "import requests\n",
    "from crewai import Agent, Task, Crew, Process, LLM\n",
    "import os \n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tool: Read website content\n",
      "\n",
      "Artificial intelligence - Wikipedia\n",
      "Jump to content\n",
      "Main menu\n",
      "Main menu\n",
      "move to sidebar\n",
      "hide\n",
      "Naviga\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tool, potentially passing the session\n",
    "scrape_tool = ScrapeWebsiteTool(website_url='https://en.wikipedia.org/wiki/Artificial_intelligence')\n",
    "\n",
    "# Extract the text\n",
    "text = scrape_tool.run()\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content saved to: ./ai.txt\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tool\n",
    "# file_writer_tool = FileWriterTool()\n",
    "\n",
    "# Write content to a file in a specified directory\n",
    "# result = file_writer_tool.run(filename='ai.txt', content = text, directory = '', overwrite=True)\n",
    "# print(result)\n",
    "\n",
    "# Set file path\n",
    "base_path = os.path.join('.', 'ai.txt')\n",
    "\n",
    "# Save file content\n",
    "with open(base_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(text)\n",
    "print(f\"Content saved to: {base_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting batches in chromadb:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load a local embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Choose a model you like\n",
    "\n",
    "# Define a function to generate embeddings\n",
    "def embed_fn(texts):\n",
    "    return embedding_model.encode(texts).tolist()\n",
    "\n",
    "ollama = LLM(\n",
    " model=\"ollama/deepseek-r1:8b\",\n",
    " base_url=\"http://host.docker.internal:11434\"\n",
    ")\n",
    "\n",
    "# Initialize the tool with a specific text file, so the agent can search within the given text file's content\n",
    "# uses chromadb to chunk and vectorize data\n",
    "search_tool = TXTSearchTool(txt='ai.txt', embed_fn=embed_fn, chunk_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 10:13:00,856 - 140093469862784 - __init__.py-__init__:537 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mEducator\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mUnderstand the topic of Natural Language Processing and summarize it for me\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mEducator\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "**\n",
      "Natural Language Processing (NLP) is a branch of computer science focused on enabling computers to understand, interpret, and generate human language. It involves processes such as tokenization, parsing, Bag of Words, and the use of machine learning algorithms to analyze text data, perform tasks like sentiment analysis, and generate responses. NLP applications include chatbots, information extraction, and text generation, utilizing tools like TensorFlow, PyTorch, NLTK, and SpaCy.\u001b[00m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_analyst = Agent(\n",
    "    role='Educator',\n",
    "    goal=f'Based on the context provided, answer the question.',\n",
    "    backstory='You are a data expert',\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=ollama,\n",
    "    tools=[search_tool]\n",
    ")\n",
    "\n",
    "test_task = Task(\n",
    "    description=\"Understand the topic of Natural Language Processing and summarize it for me\",\n",
    "    agent=data_analyst,\n",
    "    expected_output='I want the response to be as short as possible.'\n",
    ")\n",
    "\n",
    "crew = Crew(\n",
    "    tasks=[test_task],\n",
    "    process=Process.sequential,\n",
    "    model = \"ollama/deepseek-r1:8b\",\n",
    ")\n",
    "\n",
    "output = crew.kickoff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
